name: Sovereign Diagnostic Sweep

on:
  workflow_dispatch:
    inputs:
      scan_depth:
        description: 'Scan depth (all_failures, recent_only)'
        required: false
        type: choice
        options:
          - all_failures
          - recent_only
        default: 'recent_only'
      auto_fix:
        description: 'Enable auto-fix for common patterns'
        required: false
        type: boolean
        default: false
  schedule:
    # Run every 6 hours instead of 15 minutes to avoid excessive runs
    - cron: '0 */6 * * *'

permissions:
  contents: write
  actions: read
  checks: read
  pull-requests: write

jobs:
  failure-hunter:
    name: Hunt Workflow Failures
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install PyYAML tabulate
      
      - name: Scan Workflow Definitions
        id: scan-workflows
        run: |
          echo "üîç Scanning workflow definitions for common issues..."
          python3 - <<'PYTHON'
          import yaml
          import json
          import os
          from pathlib import Path
          from collections import defaultdict
          
          workflows_dir = Path(".github/workflows")
          issues = defaultdict(list)
          
          print("üìä Analyzing workflow files...")
          
          for workflow_file in workflows_dir.glob("*.yml"):
              try:
                  with open(workflow_file) as f:
                      workflow = yaml.safe_load(f)
                  
                  # Check for deprecated actions
                  workflow_str = workflow_file.read_text()
                  if "actions/upload-artifact@v3" in workflow_str:
                      issues["deprecated_actions"].append({
                          "file": str(workflow_file.relative_to(".")),
                          "issue": "Uses deprecated upload-artifact@v3",
                          "severity": "low",
                          "fix": "Update to @v4"
                      })
                  
                  if "actions/download-artifact@v3" in workflow_str:
                      issues["deprecated_actions"].append({
                          "file": str(workflow_file.relative_to(".")),
                          "issue": "Uses deprecated download-artifact@v3",
                          "severity": "low",
                          "fix": "Update to @v4"
                      })
                  
                  # Check for browser download issues
                  if "playwright install" in workflow_str or "puppeteer" in workflow_str:
                      if "PUPPETEER_SKIP" not in workflow_str and "PLAYWRIGHT_SKIP" not in workflow_str:
                          issues["browser_config"].append({
                              "file": str(workflow_file.relative_to(".")),
                              "issue": "Browser tools without skip flags",
                              "severity": "medium",
                              "fix": "Add PUPPETEER_SKIP_CHROMIUM_DOWNLOAD env var"
                          })
                  
                  # Check for missing timeout settings
                  if workflow and "jobs" in workflow:
                      for job_name, job_data in workflow.get("jobs", {}).items():
                          if isinstance(job_data, dict):
                              steps = job_data.get("steps", [])
                              for step in steps:
                                  if isinstance(step, dict):
                                      # Check for long-running commands without timeout
                                      if "run" in step and any(cmd in str(step.get("run", "")) for cmd in ["npm run build", "npm test", "pytest"]):
                                          if "timeout-minutes" not in step:
                                              issues["missing_timeouts"].append({
                                                  "file": str(workflow_file.relative_to(".")),
                                                  "job": job_name,
                                                  "issue": "Long-running command without timeout",
                                                  "severity": "low",
                                                  "fix": "Add timeout-minutes to step"
                                              })
                                              break  # Only report once per job
              
              except Exception as e:
                  print(f"‚ö†Ô∏è Error analyzing {workflow_file.name}: {e}")
          
          # Save results
          results = {
              "total_workflows": len(list(workflows_dir.glob("*.yml"))),
              "issues_by_type": {k: len(v) for k, v in issues.items()},
              "issues": dict(issues)
          }
          
          output_dir = Path("bridge_backend/diagnostics")
          output_dir.mkdir(parents=True, exist_ok=True)
          
          with open(output_dir / "workflow_scan_results.json", "w") as f:
              json.dump(results, indent=2, fp=f)
          
          print(f"\nüìä Scan Results:")
          print(f"  Total workflows: {results['total_workflows']}")
          for issue_type, count in results['issues_by_type'].items():
              print(f"  {issue_type}: {count} issues")
          
          # Output for GitHub Actions
          total_issues = sum(results['issues_by_type'].values())
          print(f"\ntotal_issues={total_issues}")
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"total_issues={total_issues}\n")
              f.write(f"has_issues={'true' if total_issues > 0 else 'false'}\n")
          
          PYTHON
      
      - name: Analyze Failure Patterns
        id: analyze-patterns
        run: |
          echo "üß† Analyzing failure patterns..."
          python3 - <<'PYTHON'
          import json
          import os
          from pathlib import Path
          from datetime import datetime, timezone
          
          # Load scan results
          scan_file = Path("bridge_backend/diagnostics/workflow_scan_results.json")
          if scan_file.exists():
              with open(scan_file) as f:
                  scan_results = json.load(f)
          else:
              scan_results = {"issues": {}}
          
          # Analyze patterns and generate fix recommendations
          patterns = {
              "browser_download_blocked": {
                  "detection": ["googlechromelabs.github.io", "storage.googleapis.com"],
                  "solution": "use_playwright_system_browsers",
                  "priority": "CRITICAL",
                  "auto_fixable": True
              },
              "deprecated_actions": {
                  "detection": ["@v3"],
                  "solution": "update_to_v4",
                  "priority": "LOW",
                  "auto_fixable": True
              },
              "missing_env_vars": {
                  "detection": ["FORGE_DOMINION_ROOT", "DOMINION_SEAL"],
                  "solution": "add_env_configuration",
                  "priority": "HIGH",
                  "auto_fixable": False
              }
          }
          
          fix_plan = {
              "timestamp": datetime.now(timezone.utc).isoformat(),
              "total_issues": sum(len(issues) for issues in scan_results.get("issues", {}).values()),
              "auto_fixable_count": 0,
              "manual_fixes_required": 0,
              "fixes": []
          }
          
          # Generate fix recommendations
          for issue_type, issues in scan_results.get("issues", {}).items():
              for issue in issues:
                  fix_item = {
                      "file": issue.get("file"),
                      "issue_type": issue_type,
                      "severity": issue.get("severity", "unknown"),
                      "description": issue.get("issue"),
                      "recommended_fix": issue.get("fix"),
                      "auto_fixable": issue.get("severity") in ["low", "medium"]
                  }
                  
                  if fix_item["auto_fixable"]:
                      fix_plan["auto_fixable_count"] += 1
                  else:
                      fix_plan["manual_fixes_required"] += 1
                  
                  fix_plan["fixes"].append(fix_item)
          
          # Save fix plan
          output_dir = Path("bridge_backend/diagnostics")
          with open(output_dir / "autofix_plan.json", "w") as f:
              json.dump(fix_plan, indent=2, fp=f)
          
          print(f"‚úÖ Fix plan generated:")
          print(f"  Total issues: {fix_plan['total_issues']}")
          print(f"  Auto-fixable: {fix_plan['auto_fixable_count']}")
          print(f"  Manual fixes: {fix_plan['manual_fixes_required']}")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"has_fixes={'true' if fix_plan['total_issues'] > 0 else 'false'}\n")
              f.write(f"auto_fixable={fix_plan['auto_fixable_count']}\n")
          
          PYTHON
      
      - name: Upload Diagnostic Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: workflow-diagnostic-results
          path: |
            bridge_backend/diagnostics/workflow_scan_results.json
            bridge_backend/diagnostics/autofix_plan.json
          retention-days: 30
      
      - name: Generate Summary Report
        if: always()
        run: |
          echo "üìã Generating diagnostic summary..."
          python3 - <<'PYTHON'
          import json
          from pathlib import Path
          
          scan_file = Path("bridge_backend/diagnostics/workflow_scan_results.json")
          fix_file = Path("bridge_backend/diagnostics/autofix_plan.json")
          
          print("\n" + "="*60)
          print("üîç SOVEREIGN DIAGNOSTIC SWEEP SUMMARY")
          print("="*60 + "\n")
          
          if scan_file.exists():
              with open(scan_file) as f:
                  scan = json.load(f)
              
              print(f"üìä Workflows Scanned: {scan.get('total_workflows', 0)}")
              print(f"\nüîß Issues Detected by Type:")
              for issue_type, count in scan.get('issues_by_type', {}).items():
                  print(f"  ‚Ä¢ {issue_type}: {count}")
          
          if fix_file.exists():
              with open(fix_file) as f:
                  fixes = json.load(f)
              
              print(f"\nüí° Fix Recommendations:")
              print(f"  ‚Ä¢ Auto-fixable: {fixes.get('auto_fixable_count', 0)}")
              print(f"  ‚Ä¢ Manual intervention: {fixes.get('manual_fixes_required', 0)}")
          
          print("\n" + "="*60)
          print("‚úÖ Diagnostic sweep complete!")
          print("="*60 + "\n")
          
          PYTHON
      
      - name: Create Issue for Manual Fixes
        if: steps.analyze-patterns.outputs.has_fixes == 'true' && steps.analyze-patterns.outputs.auto_fixable == '0'
        run: |
          echo "üìù Manual fixes required - artifact uploaded for review"
          echo "Review the workflow-diagnostic-results artifact for details"
